{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4270 Gaze Lab - Analysis of gaze patterns\n",
    "\n",
    "This notebook will show you how to analyze gaze data using video fragments of conversations obtained from the IFADV corpus.\n",
    "The raw ELAN files are already processed into JSON files for you to work with.\n",
    "\n",
    "All necessary data processing steps are already implemented for you, such that you can focus on analyzing the data.\n",
    "You will need the following packages installed in your Jupyter environment: tqdm, matplotlib, scipy, numpy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data processing\n",
    "\n",
    "Everything in this section is already implemented.\n",
    "You need to thoroughly understand what happens here in order to be able to make an analysis.\n",
    "It may also be useful to add helper functions in this section to easen your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Configure Global Variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure target files used in the analysis.\n",
    "TARGET_FILES = ['data/elan_export_dva1a_with_gaze.json', 'data/elan_export_dva6h_with_gaze.json', \n",
    "                'data/elan_export_dva4c_with_gaze.json', 'data/elan_export_dva3e_with_gaze.json',\n",
    "               'data/elan_export_dva11q_with_gaze.json', 'data/elan_export_dva14w_with_gaze.json',\n",
    "               'data/elan_export_dva22al_with_gaze.json']\n",
    "\n",
    "# Configure the detection threshold used for making the turn-taking gaze pattern plots. The default value used is from the paper: \n",
    "# Oertel, Catharine, et al. \"Gaze patterns in turn-taking.\" \n",
    "# Thirteenth annual conference of the international speech communication association. 2012.\n",
    "DETECTION_THRESHOLD_MS = 130\n",
    "\n",
    "# Configure whether to allow pre-coded prints that can make debugging easier.\n",
    "ALLOW_DEBUG_PRINT = False\n",
    "\n",
    "# Configure the random seed that is used throughout the program but more specifically for inserting artificial speakers.\n",
    "RANDOM_SEED = 0\n",
    "\n",
    "# Configure whether consecutive speaking blocks should be connected to one speaking block. \n",
    "CONNECT_SPEAKING_BLOCKS = True\n",
    "\n",
    "# Give the appropiate speakers/gaze labels associated with the TARGET_FILES. For example, the first file in \n",
    "# the array TARGET_FILES corresponds to the first item in the SPEAKER_LABELS \n",
    "# and GAZE_LABELS array. The labels are used in order\n",
    "# to identify the tiers/participants from ELAN.\n",
    "SPEAKER_LABELS = [\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE1)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"],\n",
    "    [\"spreker1 [v] (TIE0)\", \"spreker2 [v] (TIE2)\"]\n",
    "]\n",
    "\n",
    "GAZE_LABELS = [\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE2)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"],\n",
    "    [\"kijkrichting spreker1 [v] (TIE1)\", \"kijkrichting spreker2 [v] (TIE3)\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Extract ELAN JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "class ELANDataProcessing:\n",
    "    \"\"\"Responsible for extracting information from ELAN exported json files.\"\"\"\n",
    "    \n",
    "    def __init__(self, file, artificial_speakers=[], speakers=[], gazes=[], seed=RANDOM_SEED,\n",
    "                speaking_blocks=[]):\n",
    "        \"\"\"The file indicates the file used, the artificial speakers are \n",
    "        speakers added to increase the number of speakers artificially \n",
    "        (a list of string should represents their names), speakers parameter \n",
    "        is used to provide the speaker labels, gazes parameter is used to provide the gaze labels,\n",
    "        and the seed indicates which seed is used for random values.\"\"\"\n",
    "        \n",
    "        self.speakers = speakers\n",
    "        self.gazes = gazes\n",
    "        \n",
    "        self.gaze_blocks_by_speaker = {}\n",
    "        \n",
    "        for gaze_speaker in gazes:\n",
    "            self.gaze_blocks_by_speaker[gaze_speaker] = []\n",
    "        \n",
    "        self.speaking_blocks = []\n",
    "        self.ranges = {}\n",
    "        self.gaze_map = {}\n",
    "        \n",
    "        random.seed(seed)\n",
    "        \n",
    "        if len(speaking_blocks) == 0:\n",
    "            target = open(file)\n",
    "            self.data = json.load(target)\n",
    "            target.close()\n",
    "        \n",
    "            self.__extract_speaking_and_gaze_blocks()\n",
    "        else:\n",
    "            self.speaking_blocks = speaking_blocks\n",
    "        \n",
    "        for speaker in artificial_speakers:\n",
    "            self.__add_artificial_speaker(speaker)\n",
    "        \n",
    "        # Sorting by start time allows us to detect overlaps quicker and results\n",
    "        # in an array that is representative of when the utterances were said. That is,\n",
    "        # things said earlier appear early in the array and things said later appear\n",
    "        # later in the array.\n",
    "        self.speaking_blocks = sorted(self.speaking_blocks, \n",
    "                                      key=lambda b: b['time']['start'])\n",
    "        \n",
    "\n",
    "    def get_gaze_type_at_position(self, position, speaker_gaze):\n",
    "        \"\"\"Get gaze type for certain speaker at certain position ms in time.\"\"\"\n",
    "        return self.gaze_map[speaker_gaze].get(position)\n",
    "        \n",
    "\n",
    "    def get_speaking_blocks(self):\n",
    "        \"\"\"Returns all the ordered (by start time) speaking blocks.\"\"\"\n",
    "        return self.speaking_blocks\n",
    "    \n",
    "\n",
    "    def get_gaze_blocks_by_speaker(self):\n",
    "        \"\"\"Returns all the ordered (by start time) gaze blocks.\"\"\"\n",
    "        return self.gaze_blocks_by_speaker\n",
    "    \n",
    "    \n",
    "    def get_total_conversation_length(self):\n",
    "        \"\"\"Return the length of the conversation.\"\"\"\n",
    "        \n",
    "        max_end = -1\n",
    "        \n",
    "        for block in self.speaking_blocks:\n",
    "            max_end = max(max_end, block['time']['end'])\n",
    "        \n",
    "        return max_end\n",
    "\n",
    "    \n",
    "    def get_speakers(self):\n",
    "        \"\"\"Return the speakers.\"\"\"\n",
    "        return self.speakers\n",
    "    \n",
    "    \n",
    "    def get_gazes(self):\n",
    "        \"\"\"Return the gaze labels.\"\"\"\n",
    "        return self.gazes\n",
    "        \n",
    "\n",
    "    def __time_parser(self, target_selector_value):\n",
    "        \"\"\"Parses the time format given in the target selector value \n",
    "        JSON object and returns a simple object. The time values are original floats \n",
    "        representing seconds, they are converted to milliseconds for ease.\"\"\"\n",
    "        raw_values = target_selector_value[2:].split(\",\")\n",
    "\n",
    "        return {\n",
    "            'start': int(float(raw_values[0])*1000),\n",
    "            'end': int(float(raw_values[1])*1000)\n",
    "        }\n",
    "\n",
    "\n",
    "    def __add_artificial_speaker(self, name):\n",
    "        \"\"\"Adds an artificial speakers which is just a modified \n",
    "        version of the first speaker where the start and end times are slighly \n",
    "        changed with a random number.\"\"\"\n",
    "        print(f'WARNING: Artificial speaker {name} uses the first data value in \"contains\" array of the ELAN file.\\nSo make sure that is not gaze data instead of speaker data.')\n",
    "        for item in self.data['contains'][0]['first']['items']:\n",
    "            parsed_data = self.__time_parser(item['target'][0]['selector']['value'])\n",
    "\n",
    "            block = {\n",
    "                'text': item['body']['value'],\n",
    "                'time': {\n",
    "                    'start': parsed_data['start'] + random.randint(1000,2000),\n",
    "                    'end': parsed_data['end'] + random.randint(1000,2000)\n",
    "                },\n",
    "                'speaker': name\n",
    "            }\n",
    "\n",
    "            self.speaking_blocks.append(block)\n",
    "\n",
    "            \n",
    "    def __extract_speaking_and_gaze_blocks(self):\n",
    "        \"\"\"Extract speaking blocks from the original data. These blocks represent an utterance \n",
    "        and contain a id, text said, start/end time, and the speaker label.\"\"\"\n",
    "        last_index = -1\n",
    "        \n",
    "        for gazer in self.gazes:\n",
    "            self.ranges[gazer] = {\n",
    "                'g': (),\n",
    "                'x': (),\n",
    "            }\n",
    "        \n",
    "        for speaker in self.data['contains']:\n",
    "            if speaker['label'] in self.speakers:\n",
    "                for item in speaker['first']['items']:\n",
    "                    block = {\n",
    "                        'id': item['id'].split(':')[2],\n",
    "                        'text': item['body']['value'],\n",
    "                        'time': self.__time_parser(item['target'][0]['selector']['value']),\n",
    "                        'speaker': speaker['label']\n",
    "                    }\n",
    "                    \n",
    "                    if CONNECT_SPEAKING_BLOCKS:\n",
    "                        if last_index == -1:\n",
    "                            self.speaking_blocks.append(block)\n",
    "                            last_index += 1\n",
    "                            continue\n",
    "\n",
    "                        last_block = self.speaking_blocks[last_index]\n",
    "\n",
    "                        if last_block['time']['end'] == block['time']['start'] and \\\n",
    "                            last_block['speaker'] == speaker['label']:\n",
    "                            self.speaking_blocks[last_index]['text'] += (\" \" + block['text'])\n",
    "                            self.speaking_blocks[last_index]['time']['end'] = block['time']['end']\n",
    "                        else:\n",
    "                            self.speaking_blocks.append(block)\n",
    "                            last_index += 1\n",
    "                    else:\n",
    "                        self.speaking_blocks.append(block)\n",
    "            elif speaker['label'] in self.gazes:\n",
    "                for item in speaker['first']['items']:\n",
    "                    block = {\n",
    "                        'id': item['id'].split(':')[2],\n",
    "                        'label': item['body']['value'].lower(),\n",
    "                        'time': self.__time_parser(item['target'][0]['selector']['value']),\n",
    "                        'speaker_gaze': speaker['label']\n",
    "                    }\n",
    "                    \n",
    "                    # Sometimes gaze types are not 'x' or 'g', then they will automatically become 'x'.\n",
    "                    if block['label'] != 'x' and block['label'] != 'g':\n",
    "                        block['label'] = 'x'\n",
    " \n",
    "                    if block['label'] == 'x':\n",
    "                        self.ranges[speaker['label']]['x'] += ((block['time']['start'], block['time']['end']),)\n",
    "                    else:\n",
    "                        self.ranges[speaker['label']]['g'] += ((block['time']['start'], block['time']['end']),)\n",
    "\n",
    "                    self.gaze_blocks_by_speaker[speaker['label']].append(block)\n",
    "                    \n",
    "        for gazer in self.gazes:\n",
    "            # Source: https://stackoverflow.com/questions/6053974/python-efficiently-check-if-integer-is-within-many-ranges\n",
    "            self.gaze_map[gazer] = dict((k,v) for v in self.ranges[gazer] for k in chain(*map(range, *zip(*self.ranges[gazer][v]))))\n",
    "    \n",
    "\n",
    "    def get_overlaps(self):\n",
    "        \"\"\"Determine all overlaps in pairs. When there are many overlaps \n",
    "        it can be deduced which blocks are overlapping with each other.\"\"\"\n",
    "        overlaps = []\n",
    "        last_item = self.speaking_blocks[0]\n",
    "        overlap_id = 1\n",
    "\n",
    "        for item in self.speaking_blocks:\n",
    "            \n",
    "            if last_item['speaker'] == item['speaker']:\n",
    "                last_item = item\n",
    "                continue\n",
    "\n",
    "            if last_item['time']['end'] > item['time']['start'] and last_item['speaker'] != item['speaker']:\n",
    "                overlaps.append({\n",
    "                    'id': overlap_id,\n",
    "                    'overlap_time': min(last_item['time']['end'], item['time']['end']) - item['time']['start'],\n",
    "                    'last': last_item,\n",
    "                    'next': item\n",
    "                })\n",
    "\n",
    "                overlap_id += 1\n",
    "                \n",
    "            if last_item['time']['end'] < item['time']['end']:\n",
    "                last_item = item\n",
    "            \n",
    "        \n",
    "        return overlaps\n",
    "    \n",
    "    \n",
    "    def get_silence_blocks(self):\n",
    "        \"\"\"Determine moments of silence in a conversation and who started/ended them.\"\"\"\n",
    "        silence_start = 0\n",
    "        silence_blocks = []\n",
    "        silence_starter = None\n",
    "        \n",
    "        for item in self.speaking_blocks:\n",
    "            \n",
    "            if item['time']['start'] <= silence_start:\n",
    "                if item['time']['end'] > silence_start:\n",
    "                    silence_starter = item['speaker']\n",
    "                    \n",
    "                silence_start = max(item['time']['end'], silence_start)\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            if item['time']['start'] >= silence_start:\n",
    "                if ALLOW_DEBUG_PRINT:\n",
    "                    print(f\"Silence from {silence_start}ms to {item['time']['start']}ms. Starter: {silence_starter}, Ender: {item['speaker']}\")\n",
    "                \n",
    "                silence_blocks.append({\n",
    "                    'start': silence_start,\n",
    "                    'end': item['time']['start'],\n",
    "                    'duration': item['time']['start'] - silence_start,\n",
    "                    'starter': silence_starter,\n",
    "                    'ender': item['speaker'],\n",
    "                    'ender_data': item\n",
    "                })\n",
    "                    \n",
    "                silence_start = item['time']['end']\n",
    "                silence_starter = item['speaker']     \n",
    "            \n",
    "                \n",
    "        return silence_blocks\n",
    "\n",
    "# This here is just as an example and to be used for the sliding window code.\n",
    "edp = ELANDataProcessing(file=TARGET_FILES[0], speakers=SPEAKER_LABELS[0], \n",
    "                         gazes=GAZE_LABELS[0])\n",
    "speaking_blocks = edp.get_speaking_blocks()\n",
    "overlap_pairs = edp.get_overlaps()\n",
    "total_length = edp.get_total_conversation_length()\n",
    "gaze_blocks_by_speaker = edp.get_gaze_blocks_by_speaker()\n",
    "silence_blocks = edp.get_silence_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Turn-transition types and gaze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurnAndGazeAnalysis:\n",
    "    \"\"\"Responsible for detecting turn-transition types and \n",
    "    for gaze analysis at midpoints.\"\"\"\n",
    "    \n",
    "    def __init__(self, edp):\n",
    "        \"\"\"We only need an ELANDataProcessing instance for our computations.\"\"\"\n",
    "        self.overlap_pairs = edp.get_overlaps()\n",
    "        self.silence_blocks = edp.get_silence_blocks()\n",
    "        self.speakers = edp.get_speakers()\n",
    "        self.gazes = edp.get_gazes()\n",
    "        self.edp = edp\n",
    "        self.inventory = []\n",
    "        \n",
    "        self.__detect_overlap_turn_transitions()\n",
    "        self.__detect_silence_turn_transitions()\n",
    "\n",
    "        \n",
    "    def __detect_overlap_turn_transitions(self):\n",
    "        \"\"\"Determine all turn transitions with an overlap.\"\"\"\n",
    "        for overlap in self.overlap_pairs:\n",
    "            overlap_start = overlap['next']['time']['start']\n",
    "            overlap_end =  overlap['next']['time']['start'] + overlap['overlap_time']\n",
    "\n",
    "            if overlap['overlap_time'] >= DETECTION_THRESHOLD_MS:\n",
    "                if overlap['last']['time']['end'] < overlap['next']['time']['end']:\n",
    "                    #print('a. Overlap with Speaker change')\n",
    "                    self.inventory.append({\n",
    "                        'type': 'OV_with_SC',\n",
    "                        'start': overlap_start,\n",
    "                        'end': overlap_end,\n",
    "                        'time_point': overlap_start,\n",
    "                        'original_speaker': overlap['last']['speaker'],\n",
    "                        'incoming_speaker': overlap['next']['speaker']\n",
    "                    })\n",
    "\n",
    "                if overlap['last']['time']['end'] > overlap['next']['time']['end']:\n",
    "                    #print('b. Overlap without Speaker change')\n",
    "                    self.inventory.append({\n",
    "                        'type': 'OV_without_SC',\n",
    "                        'start': overlap_start,\n",
    "                        'end': overlap_end,\n",
    "                        'time_point': overlap_start,\n",
    "                        'original_speaker': overlap['last']['speaker'],\n",
    "                        'incoming_speaker': overlap['next']['speaker']\n",
    "                    })\n",
    "\n",
    "            if overlap['last']['time']['end'] > overlap['next']['time']['end'] \\\n",
    "                and overlap['overlap_time'] < DETECTION_THRESHOLD_MS:\n",
    "                #print('e. Overlap with backchannel')\n",
    "                self.inventory.append({\n",
    "                    'type': 'OV_with_BACK',\n",
    "                    'start': overlap_start,\n",
    "                    'end': overlap_end,\n",
    "                    'time_point': overlap_start,\n",
    "                    'original_speaker': overlap['last']['speaker'],\n",
    "                    'incoming_speaker': overlap['next']['speaker']\n",
    "                })\n",
    "            elif overlap['overlap_time'] < DETECTION_THRESHOLD_MS:\n",
    "                #print('g1. No-overlap')\n",
    "                self.inventory.append({\n",
    "                    'type': 'No_GAP_No_OV',\n",
    "                    'start': overlap_start,\n",
    "                    'end': overlap_end,\n",
    "                    'time_point': overlap_start,\n",
    "                    'original_speaker': overlap['last']['speaker'],\n",
    "                    'incoming_speaker': overlap['next']['speaker']\n",
    "                })\n",
    "\n",
    "\n",
    "    def __detect_silence_turn_transitions(self):\n",
    "        \"\"\"Determine all turn transitions with a silence.\"\"\"\n",
    "        for silence in self.silence_blocks:\n",
    "            if silence['starter'] != silence['ender'] \\\n",
    "                and (silence['ender_data']['time']['end'] - \\\n",
    "                     silence['ender_data']['time']['start']) < DETECTION_THRESHOLD_MS:\n",
    "                #print('f. Silence with backchannel')\n",
    "                self.inventory.append({\n",
    "                    'type': 'SIL_with_BACK',\n",
    "                    'start': silence['start'],\n",
    "                    'end': silence['end'],\n",
    "                    'time_point': silence['end'],\n",
    "                    'original_speaker': silence['starter'],\n",
    "                    'incoming_speaker': silence['ender']\n",
    "                })\n",
    "\n",
    "            if silence['duration'] >= DETECTION_THRESHOLD_MS:\n",
    "                if silence['starter'] == silence['ender']:\n",
    "                    #print('d. Silence without speaker change')\n",
    "                    \n",
    "                    # This only applies when you have two speakers.\n",
    "                    if silence['ender'] == self.speakers[0]:\n",
    "                        silence['ender'] = self.speakers[1]\n",
    "                    else:\n",
    "                        silence['ender']= self.speakers[0]\n",
    "                    \n",
    "                    self.inventory.append({\n",
    "                        'type': 'SIL_without_SC',\n",
    "                        'start': silence['start'],\n",
    "                        'end': silence['end'],\n",
    "                        'time_point': silence['end'],\n",
    "                        'original_speaker': silence['starter'],\n",
    "                        'incoming_speaker': silence['ender']\n",
    "                    })\n",
    "                else:\n",
    "                    #print('c. Silence with speaker change')\n",
    "                    self.inventory.append({\n",
    "                        'type': 'SIL_with_SC',\n",
    "                        'start': silence['start'],\n",
    "                        'end': silence['end'],\n",
    "                        'time_point': silence['end'],\n",
    "                        'original_speaker': silence['starter'],\n",
    "                        'incoming_speaker': silence['ender']\n",
    "                    })\n",
    "            else:\n",
    "                #print('g2. No-gap')\n",
    "                self.inventory.append({\n",
    "                    'type': 'No_GAP_No_OV',\n",
    "                    'start': silence['start'],\n",
    "                    'end': silence['end'],\n",
    "                    'time_point': silence['end'],\n",
    "                    'original_speaker': silence['starter'],\n",
    "                    'incoming_speaker': silence['ender']\n",
    "                })\n",
    "\n",
    "                    \n",
    "    def __opposite_gaze(self, value):\n",
    "        \"\"\"Negate gaze type.\"\"\"\n",
    "        if value == 'g':\n",
    "            return 'x'\n",
    "        else:\n",
    "            return 'g'\n",
    "        \n",
    "\n",
    "    def get_gaze_rows(self, midpoint, original_speaker, incoming_speaker):\n",
    "        \"\"\"Construct gaze rows given midpoint and the original/incoming speaker.\"\"\"\n",
    "        left = midpoint - 3000\n",
    "        right = midpoint + 3000\n",
    "\n",
    "        if left < 0 or right > total_length:\n",
    "            return None, None\n",
    "\n",
    "        gaze_index_og = self.speakers.index(original_speaker)\n",
    "        gaze_index_in = self.speakers.index(incoming_speaker)\n",
    "\n",
    "        row_og = \"\"\n",
    "        row_in = \"\"\n",
    "        last_og = None\n",
    "        last_in = None\n",
    "\n",
    "        for i in range(left, right, 10):\n",
    "            original = self.edp.get_gaze_type_at_position(i, self.gazes[gaze_index_og])\n",
    "            incoming = self.edp.get_gaze_type_at_position(i, self.gazes[gaze_index_in])\n",
    "\n",
    "            # Sometimes we can get None as the gaze type if gaze position\n",
    "            # does not fall in an gaze interval (block). When this occurs the opposite value of\n",
    "            # the last gaze type will be used because the missing interval likely occurs\n",
    "            # at a gaze type switch moment (e.g. going from g to x). \n",
    "            if not original:\n",
    "                original = self.__opposite_gaze(last_og)\n",
    "\n",
    "            if not incoming:\n",
    "                incoming = self.__opposite_gaze(last_in)\n",
    "\n",
    "            row_og += original\n",
    "            row_in += incoming\n",
    "\n",
    "        return row_og, row_in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.4 Collect data from conversations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [00:04<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: run the next cell in the notebook after this cell has completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "plot_data = {\n",
    "    'OV_with_SC': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'OV_without_SC': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'OV_with_BACK': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'SIL_with_BACK': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'SIL_without_SC': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'SIL_with_SC': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    },\n",
    "    'No_GAP_No_OV': {\n",
    "        'incoming': [],\n",
    "        'original': []\n",
    "    }\n",
    "}\n",
    "\n",
    "for i in tqdm.tqdm(range(len(TARGET_FILES))):\n",
    "    edp = ELANDataProcessing(file=TARGET_FILES[i], speakers=SPEAKER_LABELS[i], \n",
    "                             gazes=GAZE_LABELS[i])\n",
    "    conv = TurnAndGazeAnalysis(edp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Research questions\n",
    "\n",
    "If we want to know what should be analysed, we should know what questions we want to answer first.\n",
    "Try to come up with some research questions that will help us create a more realistic virtual starship receptionist in the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Data visualisation\n",
    "\n",
    "For each question you constructed, create one or more plots that help you answer that question.\n",
    "You can find some inspiration in [Oertel, Catharine, et al. \"Gaze patterns in turn-taking.\"](https://www.speech.kth.se/prod/publications/files/3744.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Modelling gaze\n",
    "\n",
    "If all went well, you should have obtained some distributions of gaze over time in different situations.\n",
    "In this section, we will think about how we can use this information to model the gaze of our virtual receptionist.\n",
    "This requires you to first come up with variables that you will need to implement, e.g. the amount of seconds after a person starts talking until the direction of gaze changes.\n",
    "For simplicity, you can model these variables using Gaussian distributions. (Of course you are allowed to make as complicated a model as you want.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
